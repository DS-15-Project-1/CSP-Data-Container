{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_and_merge_parquet_files(directory):\n",
    "    \"\"\"\n",
    "    Reads all parquet files from the specified directory and its subdirectories,\n",
    "    merges them into a single DataFrame, and returns it.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): Path to the directory containing parquet files.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A single DataFrame object representing concatenated data from all parquet files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty list to store the DataFrame objects\n",
    "    dataframes = []\n",
    "\n",
    "    # Function to find all files recursively\n",
    "    def find_all_files(directory):\n",
    "        all_files = []\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                all_files.append(file_path)\n",
    "        return all_files\n",
    "\n",
    "    # Find all Parquet files in the directory structure\n",
    "    all_files = find_all_files(directory)\n",
    "\n",
    "    print(f\"\\nTotal number of files found: {len(all_files)}\")\n",
    "\n",
    "    if not all_files:\n",
    "        print(\"\\nNo files found in any directory.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nFound {len(all_files)} files in {directory}\")\n",
    "\n",
    "    # Process files in batches\n",
    "    batch_size = 10\n",
    "    for i in range(0, len(all_files), batch_size):\n",
    "        try:\n",
    "            batch_files = all_files[i:i + batch_size]\n",
    "            batch = [pd.read_parquet(filepath) for filepath in batch_files]\n",
    "            dataframes.extend(batch)\n",
    "            print(f\"Batch {i//batch_size + 1} processed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {str(e)}\")\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    result_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Display information about the resulting DataFrame\n",
    "    print(\"\\nDataFrame Information:\")\n",
    "    print(result_df.info())\n",
    "    print(result_df.head())\n",
    "    print(result_df.tail())\n",
    "    \n",
    "\n",
    "    # Print some statistics\n",
    "    print(\"\\nData Statistics:\")\n",
    "    print(result_df.describe())\n",
    "\n",
    "    return result_df.to_csv('merged_seismic_data2.csv')\n",
    "\n",
    "# Usage\n",
    "directory = '/notebooks/Mine-folder/output/FWU3/HHE.D_Partial'\n",
    "merged_dataframe = read_and_merge_parquet_files(directory)\n",
    "\n",
    "if merged_dataframe is not None:\n",
    "    print(\"Merged DataFrame shape:\", merged_dataframe.shape)\n",
    "else:\n",
    "    print(\"Failed to merge DataFrames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of files found: 33\n",
      "\n",
      "Found 33 files in /notebooks/Mine-folder/output/FWU3/HHE.D\n",
      "Batch 1 processed.\n",
      "Batch 2 processed.\n",
      "Batch 3 processed.\n",
      "Batch 4 processed.\n",
      "\n",
      "DataFrame Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33 entries, 0 to 32\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   network        33 non-null     object \n",
      " 1   station        33 non-null     object \n",
      " 2   location       33 non-null     object \n",
      " 3   channel        33 non-null     object \n",
      " 4   starttime      33 non-null     object \n",
      " 5   endtime        33 non-null     object \n",
      " 6   sampling_rate  33 non-null     float64\n",
      " 7   data           33 non-null     object \n",
      "dtypes: float64(1), object(7)\n",
      "memory usage: 2.2+ KB\n",
      "None\n",
      "  network station location channel            starttime  \\\n",
      "0      ZZ    FWU3       10     HHE  2019-12-26T00:00:00   \n",
      "1      ZZ    FWU3       10     HHE  2019-11-02T00:00:00   \n",
      "2      ZZ    FWU3       10     HHE  2019-12-22T00:00:00   \n",
      "3      ZZ    FWU3       10     HHE  2019-12-15T00:00:00   \n",
      "4      ZZ    FWU3       10     HHE  2019-11-28T00:00:00   \n",
      "\n",
      "                      endtime  sampling_rate  \\\n",
      "0  2019-12-26T23:59:59.996000          250.0   \n",
      "1  2019-11-02T23:59:59.996000          250.0   \n",
      "2  2019-12-22T23:59:59.996000          250.0   \n",
      "3  2019-12-15T23:59:59.996000          250.0   \n",
      "4  2019-11-28T23:59:59.996000          250.0   \n",
      "\n",
      "                                                data  \n",
      "0  [-2718, -384, -1595, -1751, -2711, 144, -2262,...  \n",
      "1  [1313, 1352, 1389, 1436, 1223, 1200, 1302, 126...  \n",
      "2  [-7410, -5221, 3344, 4025, -270, -87, -461, -6...  \n",
      "3  [-11938, -1006, 11098, 6046, -13131, -8026, 38...  \n",
      "4  [-6507, -5165, -3379, 2658, 3588, 5219, 3267, ...  \n",
      "   network station location channel            starttime  \\\n",
      "28      ZZ    FWU3       10     HHE  2019-12-23T00:00:00   \n",
      "29      ZZ    FWU3       10     HHE  2019-12-06T00:00:00   \n",
      "30      ZZ    FWU3       10     HHE  2019-11-09T00:00:00   \n",
      "31      ZZ    FWU3       10     HHE  2019-11-30T00:00:00   \n",
      "32      ZZ    FWU3       10     HHE  2019-11-11T00:00:00   \n",
      "\n",
      "                       endtime  sampling_rate  \\\n",
      "28  2019-12-23T23:59:59.996000          250.0   \n",
      "29  2019-12-06T23:59:59.996000          250.0   \n",
      "30  2019-11-09T23:59:59.996000          250.0   \n",
      "31  2019-11-30T23:59:59.996000          250.0   \n",
      "32  2019-11-11T23:59:59.996000          250.0   \n",
      "\n",
      "                                                 data  \n",
      "28  [11530, 6190, 242, -4298, -7735, -11419, -1230...  \n",
      "29  [-330, -667, -1103, -1776, -1944, -1860, -1976...  \n",
      "30  [-1073, -1324, -2836, -3468, -3491, -4012, -32...  \n",
      "31  [505, 1121, 1357, 615, -267, -964, -1147, -510...  \n",
      "32  [-1055, -4816, -3984, 2404, 4964, -1789, -8827...  \n",
      "\n",
      "Data Statistics:\n",
      "       sampling_rate\n",
      "count           33.0\n",
      "mean           250.0\n",
      "std              0.0\n",
      "min            250.0\n",
      "25%            250.0\n",
      "50%            250.0\n",
      "75%            250.0\n",
      "max            250.0\n",
      "Failed to merge DataFrames.\n"
     ]
    }
   ],
   "source": [
    " import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_and_merge_parquet_files(directory):\n",
    "    \"\"\"\n",
    "    Reads all parquet files from the specified directory and its subdirectories,\n",
    "    merges them into a single DataFrame, and returns it.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): Path to the directory containing parquet files.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A single DataFrame object representing concatenated data from all parquet files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty list to store the DataFrame objects\n",
    "    dataframes = []\n",
    "\n",
    "    # Function to find all files recursively\n",
    "    def find_all_files(directory):\n",
    "        all_files = []\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                all_files.append(file_path)\n",
    "        return all_files\n",
    "\n",
    "    # Find all Parquet files in the directory structure\n",
    "    all_files = find_all_files(directory)\n",
    "\n",
    "    print(f\"\\nTotal number of files found: {len(all_files)}\")\n",
    "\n",
    "    if not all_files:\n",
    "        print(\"\\nNo files found in any directory.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nFound {len(all_files)} files in {directory}\")\n",
    "\n",
    "    # Process files in batches\n",
    "    batch_size = 10\n",
    "    for i in range(0, len(all_files), batch_size):\n",
    "        try:\n",
    "            batch_files = all_files[i:i + batch_size]\n",
    "            batch = [pd.read_parquet(filepath) for filepath in batch_files]\n",
    "            dataframes.extend(batch)\n",
    "            print(f\"Batch {i//batch_size + 1} processed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {str(e)}\")\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    result_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Display information about the resulting DataFrame\n",
    "    print(\"\\nDataFrame Information:\")\n",
    "    print(result_df.info())\n",
    "    print(result_df.head())\n",
    "    print(result_df.tail())\n",
    "    \n",
    "\n",
    "    # Print some statistics\n",
    "    print(\"\\nData Statistics:\")\n",
    "    print(result_df.describe())\n",
    "\n",
    "    return result_df.to_csv('merged_seismic_data1.csv')\n",
    "\n",
    "# Usage\n",
    "directory = '/notebooks/Mine-folder/output/FWU3/HHE.D'\n",
    "merged_dataframe = read_and_merge_parquet_files(directory)\n",
    "\n",
    "if merged_dataframe is not None:\n",
    "    print(\"Merged DataFrame shape:\", merged_dataframe.shape)\n",
    "else:\n",
    "    print(\"Failed to merge DataFrames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of files found: 25\n",
      "\n",
      "Found 25 files in /notebooks/Mine-folder/output/FWU3/HHE.D_Partial\n",
      "Batch 1 processed.\n",
      "Batch 2 processed.\n",
      "Batch 3 processed.\n",
      "\n",
      "DataFrame Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25 entries, 0 to 24\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   network        25 non-null     object \n",
      " 1   station        25 non-null     object \n",
      " 2   location       25 non-null     object \n",
      " 3   channel        25 non-null     object \n",
      " 4   starttime      25 non-null     object \n",
      " 5   endtime        25 non-null     object \n",
      " 6   sampling_rate  25 non-null     float64\n",
      " 7   data           25 non-null     object \n",
      "dtypes: float64(1), object(7)\n",
      "memory usage: 1.7+ KB\n",
      "None\n",
      "  network station location channel            starttime  \\\n",
      "0      ZZ    FWU3       10     HHE  2019-12-04T00:00:00   \n",
      "1      ZZ    FWU3       10     HHE  2019-12-18T00:00:00   \n",
      "2      ZZ    FWU3       10     HHE  2019-12-07T00:00:00   \n",
      "3      ZZ    FWU3       10     HHE  2019-11-22T00:00:00   \n",
      "4      ZZ    FWU3       10     HHE  2019-11-18T00:00:00   \n",
      "\n",
      "                      endtime  sampling_rate  \\\n",
      "0  2019-12-04T23:59:59.996000          250.0   \n",
      "1  2019-12-18T23:59:59.996000          250.0   \n",
      "2  2019-12-07T23:59:59.996000          250.0   \n",
      "3  2019-11-22T23:59:59.996000          250.0   \n",
      "4  2019-11-18T23:59:59.996000          250.0   \n",
      "\n",
      "                                                data  \n",
      "0  [-9043, -8798, -7143, 1588, 13054, 14412, 5043...  \n",
      "1  [-2976, -11244, 15436, 24815, 12492, -1528, -9...  \n",
      "2  [-1039, -2527, -3052, -2109, 425, 3623, 5305, ...  \n",
      "3  [-2097, -1911, -2522, -737, -1254, -3507, -161...  \n",
      "4  [-4618, -2866, -3711, -2151, 1939, 4271, 1736,...  \n",
      "   network station location channel            starttime  \\\n",
      "20      ZZ    FWU3       10     HHE  2019-12-20T00:00:00   \n",
      "21      ZZ    FWU3       10     HHE  2019-12-03T00:00:00   \n",
      "22      ZZ    FWU3       10     HHE  2019-10-31T00:00:00   \n",
      "23      ZZ    FWU3       10     HHE  2019-11-17T00:00:00   \n",
      "24      ZZ    FWU3       10     HHE  2019-11-16T00:00:00   \n",
      "\n",
      "                       endtime  sampling_rate  \\\n",
      "20  2019-12-20T23:59:59.996000          250.0   \n",
      "21  2019-12-03T23:59:59.996000          250.0   \n",
      "22  2019-10-31T23:59:59.996000          250.0   \n",
      "23  2019-11-17T23:59:59.996000          250.0   \n",
      "24  2019-11-16T23:59:59.996000          250.0   \n",
      "\n",
      "                                                 data  \n",
      "20  [1250, 1737, 2299, 3148, 3805, 3864, 3747, 282...  \n",
      "21  [-1099, -1461, -1416, -1856, -1807, -1302, -15...  \n",
      "22  [-1258, -1252, -1932, -2122, -2434, -1765, -10...  \n",
      "23  [-283, -240, -45, -288, -67, -36, -314, -345, ...  \n",
      "24  [-114, 1935, 2881, 2064, -198, -2699, -4123, -...  \n",
      "\n",
      "Data Statistics:\n",
      "       sampling_rate\n",
      "count           25.0\n",
      "mean           250.0\n",
      "std              0.0\n",
      "min            250.0\n",
      "25%            250.0\n",
      "50%            250.0\n",
      "75%            250.0\n",
      "max            250.0\n",
      "Failed to merge DataFrames.\n"
     ]
    }
   ],
   "source": [
    " import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_and_merge_parquet_files(directory):\n",
    "    \"\"\"\n",
    "    Reads all parquet files from the specified directory and its subdirectories,\n",
    "    merges them into a single DataFrame, and returns it.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): Path to the directory containing parquet files.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A single DataFrame object representing concatenated data from all parquet files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty list to store the DataFrame objects\n",
    "    dataframes = []\n",
    "\n",
    "    # Function to find all files recursively\n",
    "    def find_all_files(directory):\n",
    "        all_files = []\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                all_files.append(file_path)\n",
    "        return all_files\n",
    "\n",
    "    # Find all Parquet files in the directory structure\n",
    "    all_files = find_all_files(directory)\n",
    "\n",
    "    print(f\"\\nTotal number of files found: {len(all_files)}\")\n",
    "\n",
    "    if not all_files:\n",
    "        print(\"\\nNo files found in any directory.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nFound {len(all_files)} files in {directory}\")\n",
    "\n",
    "    # Process files in batches\n",
    "    batch_size = 10\n",
    "    for i in range(0, len(all_files), batch_size):\n",
    "        try:\n",
    "            batch_files = all_files[i:i + batch_size]\n",
    "            batch = [pd.read_parquet(filepath) for filepath in batch_files]\n",
    "            dataframes.extend(batch)\n",
    "            print(f\"Batch {i//batch_size + 1} processed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {str(e)}\")\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    result_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Display information about the resulting DataFrame\n",
    "    print(\"\\nDataFrame Information:\")\n",
    "    print(result_df.info())\n",
    "    print(result_df.head())\n",
    "    print(result_df.tail())\n",
    "    \n",
    "\n",
    "    # Print some statistics\n",
    "    print(\"\\nData Statistics:\")\n",
    "    print(result_df.describe())\n",
    "\n",
    "    return result_df.to_csv('merged_seismic_data2.csv')\n",
    "\n",
    "# Usage\n",
    "directory = '/notebooks/Mine-folder/output/FWU3/HHE.D_Partial'\n",
    "merged_dataframe = read_and_merge_parquet_files(directory)\n",
    "\n",
    "if merged_dataframe is not None:\n",
    "    print(\"Merged DataFrame shape:\", merged_dataframe.shape)\n",
    "else:\n",
    "    print(\"Failed to merge DataFrames.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
