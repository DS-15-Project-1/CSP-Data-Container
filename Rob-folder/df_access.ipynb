{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of files found: 0\n",
      "\n",
      "No files found in any directory.\n",
      "Failed to merge DataFrames.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_and_merge_parquet_files(directory):\n",
    "    \"\"\"\n",
    "    Reads all parquet files from the specified directory and its subdirectories,\n",
    "    merges them into a single DataFrame, and returns it.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): Path to the directory containing parquet files.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A single DataFrame object representing concatenated data from all parquet files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty list to store the DataFrame objects\n",
    "    dataframes = []\n",
    "\n",
    "    # Function to find all files recursively\n",
    "    def find_all_files(directory):\n",
    "        all_files = []\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                all_files.append(file_path)\n",
    "        return all_files\n",
    "\n",
    "    # Find all Parquet files in the directory structure\n",
    "    all_files = find_all_files(directory)\n",
    "\n",
    "    print(f\"\\nTotal number of files found: {len(all_files)}\")\n",
    "\n",
    "    if not all_files:\n",
    "        print(\"\\nNo files found in any directory.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nFound {len(all_files)} files in {directory}\")\n",
    "\n",
    "    # Process files in batches\n",
    "    batch_size = 10\n",
    "    for i in range(0, len(all_files), batch_size):\n",
    "        try:\n",
    "            batch_files = all_files[i:i + batch_size]\n",
    "            batch = [pd.read_parquet(filepath) for filepath in batch_files]\n",
    "            dataframes.extend(batch)\n",
    "            print(f\"Batch {i//batch_size + 1} processed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {str(e)}\")\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    result_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Display information about the resulting DataFrame\n",
    "    print(\"\\nDataFrame Information:\")\n",
    "    print(result_df.info())\n",
    "    print(result_df.head())\n",
    "    print(result_df.tail())\n",
    "\n",
    "    # Print some statistics\n",
    "    print(\"\\nData Statistics:\")\n",
    "    print(result_df.describe())\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Usage\n",
    "directory = '/mnt/f/parquet/ZZ/FWU1/HHE.D'\n",
    "merged_dataframe = read_and_merge_parquet_files(directory)\n",
    "\n",
    "if merged_dataframe is not None:\n",
    "    print(\"Merged DataFrame shape:\", merged_dataframe.shape)\n",
    "else:\n",
    "    print(\"Failed to merge DataFrames.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
